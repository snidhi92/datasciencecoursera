install.packages(c("dplyr", "RWeka"))
install.packages("RWeka")
knitr::opts_chunk$set(echo = TRUE)
fileUrl <-"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
if (!file.exists("Coursera-SwiftKey.zip")){
download.file(fileUrl, destfile = "Coursera-SwiftKey.zip")
}
unzip("Coursera-SwiftKey.zip")
install.packages("RWeka")
file.list = c("final/en_US/en_US.blogs.txt", "final/en_US/en_US.news.txt", "final/en_US/en_US.twitter.txt")
text <- list(blogs = "", news = "", twitter = "")
data.summary <- matrix(0, nrow = 3, ncol = 3, dimnames = list(c("blogs", "news", "twitter"),c("file size, Mb", "lines", "words")))
for (i in 1:3) {
con <- file(file.list[i], "rb")
text[[i]] <- readLines(con, encoding = "UTF-8",skipNul = TRUE)
close(con)
data.summary[i,1] <- round(file.info(file.list[i])$size / 1024^2, 2)
data.summary[i,2] <- length(text[[i]])
data.summary[i,3] <- sum(stri_count_words(text[[i]]))
}
fileUrl <-"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
if (!file.exists("Coursera-SwiftKey.zip")){
download.file(fileUrl, destfile = "Coursera-SwiftKey.zip")
}
unzip("Coursera-SwiftKey.zip")
fileUrl <-"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
if (!file.exists("Coursera-SwiftKey.zip")){
download.file(fileUrl, destfile = "Coursera-SwiftKey.zip")
}
unzip("Coursera-SwiftKey.zip")
file.list = c("final/en_US/en_US.blogs.txt", "final/en_US/en_US.news.txt", "final/en_US/en_US.twitter.txt")
text <- list(blogs = "", news = "", twitter = "")
data.summary <- matrix(0, nrow = 3, ncol = 3, dimnames = list(c("blogs", "news", "twitter"),c("file size, Mb", "lines", "words")))
for (i in 1:3) {
con <- file(file.list[i], "rb")
text[[i]] <- readLines(con, encoding = "UTF-8",skipNul = TRUE)
close(con)
data.summary[i,1] <- round(file.info(file.list[i])$size / 1024^2, 2)
data.summary[i,2] <- length(text[[i]])
data.summary[i,3] <- sum(stri_count_words(text[[i]]))
}
file.list = c("final/en_US/en_US.blogs.txt", "final/en_US/en_US.news.txt", "final/en_US/en_US.twitter.txt")
text <- list(blogs = "", news = "", twitter = "")
matrix.summary <- matrix(0, nrow = 3, ncol = 3, dimnames = list(c("blogs", "news", "twitter"),c("file size, Mb", "lines", "words")))
for (i in 1:3) {
con <- file(file.list[i], "rb")
text[[i]] <- readLines(con, encoding = "UTF-8",skipNul = TRUE)
close(con)
matrix.summary[i,1] <- round(file.info(file.list[i])$size / 1024^2, 2)
matrix.summary[i,2] <- length(text[[i]])
matrix.summary[i,3] <- sum(stri_count_words(text[[i]]))
}
install.packages("RWeka")
library(dplyr)
library(RWekajars)
file.list = c("final/en_US/en_US.blogs.txt", "final/en_US/en_US.news.txt", "final/en_US/en_US.twitter.txt")
text <- list(blogs = "", news = "", twitter = "")
data.summary <- matrix(0, nrow = 3, ncol = 3, dimnames = list(c("blogs", "news", "twitter"),c("file size, Mb", "lines", "words")))
for (i in 1:3) {
con <- file(file.list[i], "rb")
text[[i]] <- readLines(con, encoding = "UTF-8",skipNul = TRUE)
close(con)
data.summary[i,1] <- round(file.info(file.list[i])$size / 1024^2, 2)
data.summary[i,2] <- length(text[[i]])
data.summary[i,3] <- sum(stri_count_words(text[[i]]))
}
install.packages("RWeka")
list.of.packages <- c("stringi", "tm", "wordcloud", "RColorBrewer")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, repos="http://cran.rstudio.com/")
library(dplyr)
file.list = c("final/en_US/en_US.blogs.txt", "final/en_US/en_US.news.txt", "final/en_US/en_US.twitter.txt")
text <- list(blogs = "", news = "", twitter = "")
data.summary <- matrix(0, nrow = 3, ncol = 3, dimnames = list(c("blogs", "news", "twitter"),c("file size, Mb", "lines", "words")))
for (i in 1:3) {
con <- file(file.list[i], "rb")
text[[i]] <- readLines(con, encoding = "UTF-8",skipNul = TRUE)
close(con)
data.summary[i,1] <- round(file.info(file.list[i])$size / 1024^2, 2)
data.summary[i,2] <- length(text[[i]])
stri_count_words(text[[i]])
data.summary[i,3] <- sum(stri_count_words(text[[i]]))
}
library(stringi)
file.list = c("final/en_US/en_US.blogs.txt", "final/en_US/en_US.news.txt", "final/en_US/en_US.twitter.txt")
text <- list(blogs = "", news = "", twitter = "")
data.summary <- matrix(0, nrow = 3, ncol = 3, dimnames = list(c("blogs", "news", "twitter"),c("file size, Mb", "lines", "words")))
for (i in 1:3) {
con <- file(file.list[i], "rb")
text[[i]] <- readLines(con, encoding = "UTF-8",skipNul = TRUE)
close(con)
data.summary[i,1] <- round(file.info(file.list[i])$size / 1024^2, 2)
data.summary[i,2] <- length(text[[i]])
stri_count_words(text[[i]])
data.summary[i,3] <- sum(stri_count_words(text[[i]]))
}
library(knitr)
kable(data.summary)
detach("package:RWekajars", unload = TRUE)
install.packages("RWeka")
install.packages("rJava")
install.packages("RWeka")
system("java -version")
install.packages("RWeka")
system("java -version")
install.packages("rJava")
installed.packages()
install.packages("RWeka")
library(RWekajars)
detach("package:RWekajars", unload = TRUE)
system("R -version")
system("java -version")
R CMD javareconf
Sys.getenv("R_ARCH")
install.packages("RWeka")
system("java -version")
install.packages("rJava", type="source")
install.packages("RWeka", type="source")
Sys.setenv('JAVA_HOME'="C:/Program Files/Java/jdk-14.0.2/")
install.packages("rJava", type="source")
install.packages("RWeka", type="source")
install.packages("RWeka")
install.packages("ggplot2")
knitr::opts_chunk$set(echo = TRUE)
# Create corpus
gc()
corpus3 <- Corpus(VectorSource(twitter_sample))
library(tm)
library(wordcloud)
library(RColorBrewer)
# remove emoticons
sampled_data <- iconv(sampled_data, 'UTF-8', 'ASCII')
term.doc.matrix1 <- TermDocumentMatrix(corpus1)
fileUrl <-"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
if (!file.exists("Coursera-SwiftKey.zip")){
download.file(fileUrl, destfile = "Coursera-SwiftKey.zip")
}
unzip("Coursera-SwiftKey.zip")
file.list = c("final/en_US/en_US.blogs.txt", "final/en_US/en_US.news.txt", "final/en_US/en_US.twitter.txt")
text <- list(blogs = "", news = "", twitter = "")
matrix.summary <- matrix(0, nrow = 3, ncol = 3, dimnames = list(c("blogs", "news", "twitter"),c("file size, Mb", "lines", "words")))
for (i in 1:3) {
con <- file(file.list[i], "rb")
text[[i]] <- readLines(con, encoding = "UTF-8",skipNul = TRUE)
close(con)
matrix.summary[i,1] <- round(file.info(file.list[i])$size / 1024^2, 2)
matrix.summary[i,2] <- length(text[[i]])
matrix.summary[i,3] <- sum(stri_count_words(text[[i]]))
}
list.of.packages <- c("stringi", "tm", "wordcloud", "RColorBrewer")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, repos="http://cran.rstudio.com/")
library(dplyr)
library(ggplot2)
library(stringi)
fileUrl <-"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
if (!file.exists("Coursera-SwiftKey.zip")){
download.file(fileUrl, destfile = "Coursera-SwiftKey.zip")
}
unzip("Coursera-SwiftKey.zip")
file.list = c("final/en_US/en_US.blogs.txt", "final/en_US/en_US.news.txt", "final/en_US/en_US.twitter.txt")
text <- list(blogs = "", news = "", twitter = "")
matrix.summary <- matrix(0, nrow = 3, ncol = 3, dimnames = list(c("blogs", "news", "twitter"),c("file size, Mb", "lines", "words")))
for (i in 1:3) {
con <- file(file.list[i], "rb")
text[[i]] <- readLines(con, encoding = "UTF-8",skipNul = TRUE)
close(con)
matrix.summary[i,1] <- round(file.info(file.list[i])$size / 1024^2, 2)
matrix.summary[i,2] <- length(text[[i]])
matrix.summary[i,3] <- sum(stri_count_words(text[[i]]))
}
library(knitr)
kable(matrix.summary)
set.seed(123)
blogs_sample <- sample(text$blogs, 0.01*length(text$blogs))
news_sample <- sample(text$news, 0.01*length(text$news))
twitter_sample <- sample(text$twitter, 0.01*length(text$twitter))
sampled_data <- c(blogs_sample, news_sample, twitter_sample)
sum <- sum(stri_count_words(sampled_data))
sum
library(tm)
library(wordcloud)
library(RColorBrewer)
# remove emoticons
sampled_data <- iconv(sampled_data, 'UTF-8', 'ASCII')
# Create corpus
corpus1 <- Corpus(VectorSource(blogs_sample))
# To lower case
corpus1 <- tm_map(corpus1, content_transformer(tolower))
# Remove punctuation marks
corpus1 <- tm_map(corpus1, removePunctuation)
# Remove numbers
corpus1 <- tm_map(corpus1, removeNumbers)
#remove stop words
corpus1 <- tm_map(corpus1, removeWords, stopwords("english"))
#Remove whitespaces
corpus1 <- tm_map(corpus1, stripWhitespace)
frequentWords <- head(sort(rowSums(as.matrix(TermDocumentMatrix(corpus1))),decreasing=TRUE), 10)
barplot(frequentWords,
main = "Blogs Data: Most Frequent Words",
xlab="Word",
ylab = "Count")
term.doc.matrix1 <- TermDocumentMatrix(corpus1)
term.doc.matrix1 <- as.matrix(term.doc.matrix1)
word.freqs1 <- sort(rowSums(term.doc.matrix1), decreasing=TRUE)
dm1 <- data.frame(word=names(word.freqs1), freq=word.freqs1)
term.doc.matrix1 <- TermDocumentMatrix(corpus1)
term.doc.matrix1 <- as.matrix(term.doc.matrix1)
word.freqs1 <- sort(rowSums(term.doc.matrix1), decreasing=TRUE)
dm1 <- data.frame(word=names(word.freqs1), freq=word.freqs1)
wordcloud(dm1$word, dm1$freq, min.freq= 150,scale=c(4,.5), random.order=TRUE, rot.per=.15, colors=brewer.pal(8, "Dark2"))
# Create corpus
corpus2 <- Corpus(VectorSource(news_sample))
# To lower case
corpus2 <- tm_map(corpus2, content_transformer(tolower))
# Remove punctuation marks
corpus2 <- tm_map(corpus2, removePunctuation)
# Remove numbers
corpus2 <- tm_map(corpus2, removeNumbers)
#remove stop words
corpus2 <- tm_map(corpus2, removeWords, stopwords("english"))
#Remove whitespaces
corpus2 <- tm_map(corpus2, stripWhitespace)
frequentWords <- head(sort(rowSums(as.matrix(TermDocumentMatrix(corpus2))),decreasing=TRUE), 10)
barplot(frequentWords,
main = "News Data: Most Frequent Words",
xlab="Word",
ylab = "Count")
term.doc.matrix2 <- TermDocumentMatrix(corpus2)
term.doc.matrix2 <- as.matrix(term.doc.matrix2)
word.freqs2 <- sort(rowSums(term.doc.matrix2), decreasing=TRUE)
dm2 <- data.frame(word=names(word.freqs2), freq=word.freqs2)
wordcloud(dm2$word, dm2$freq, min.freq= 100, random.order=TRUE, rot.per=.25, colors=brewer.pal(8, "Dark2"))
# Create corpus
gc()
corpus3 <- Corpus(VectorSource(twitter_sample))
## Convert Character Vector between Encodings
corpus3 <- tm_map(corpus3, content_transformer(function(x)
iconv(x, to = "UTF-8", sub = "byte")))
# To lower case
corpus3 <- tm_map(corpus3, content_transformer(tolower))
# Remove punctuation marks
corpus3 <- tm_map(corpus3, removePunctuation)
# Remove numbers
corpus3 <- tm_map(corpus3, removeNumbers)
#remove stop words
corpus3 <- tm_map(corpus3, removeWords, stopwords("english"))
#Remove whitespaces
corpus3 <- tm_map(corpus3, stripWhitespace)
frequentWords <- head(sort(rowSums(as.matrix(TermDocumentMatrix(corpus3))),decreasing=TRUE), 10)
memory.size()
Sys.getenv("R_ARCH")
Sys.info()[["machine"]]
memory.limit(size=56000)
# Create corpus
gc()
corpus3 <- Corpus(VectorSource(twitter_sample))
## Convert Character Vector between Encodings
corpus3 <- tm_map(corpus3, content_transformer(function(x)
iconv(x, to = "UTF-8", sub = "byte")))
# To lower case
corpus3 <- tm_map(corpus3, content_transformer(tolower))
# Remove punctuation marks
corpus3 <- tm_map(corpus3, removePunctuation)
# Remove numbers
corpus3 <- tm_map(corpus3, removeNumbers)
#remove stop words
corpus3 <- tm_map(corpus3, removeWords, stopwords("english"))
#Remove whitespaces
corpus3 <- tm_map(corpus3, stripWhitespace)
frequentWords <- head(sort(rowSums(as.matrix(TermDocumentMatrix(corpus3))),decreasing=TRUE), 10)
barplot(frequentWords,
main = "Twitter Data: Most Frequent Words",
xlab="Word",
ylab = "Count")
term.doc.matrix3 <- TermDocumentMatrix(corpus3)
term.doc.matrix3 <- as.matrix(term.doc.matrix3)
install.packages("RWeka")
shiny::runApp('D:/Cousera/datasciencecoursera/Capstone')
